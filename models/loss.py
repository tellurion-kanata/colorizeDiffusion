import torch
import torch.nn as nn

def gap(x: torch.Tensor = None, keepdim=True):
    if len(x.shape) == 4:
        return torch.mean(x, dim=[2, 3], keepdim=keepdim)
    elif len(x.shape) == 3:
        return torch.mean(x, dim=[1], keepdim=keepdim)
    else:
        raise NotImplementedError('gap input should be 3d or 4d tensors')

def sine_loss(fv, ov, t):
    """
        Compute the loss along the sine direction.
        fv: fake visual features generated by mapper network
        ov: original visual features
    """
    # fv, ov = map(lambda t: t/t.norm(dim=2, keepdim=True), (fv, ov))
    f_proj = fv @ t.mT
    o_proj = ov @ t.mT

    sin_fv = (fv ** 2.).sum(dim=2, keepdim=True) - f_proj ** 2.
    sin_ov = (ov ** 2.).sum(dim=2, keepdim=True) - o_proj ** 2.
    return torch.abs(sin_ov - sin_fv)


class MappingLoss(nn.Module):
    def __init__(self, target, trigular_weight=1.0, reconstruction_weight=1., loss_type="l1"):
        super().__init__()
        self.trigular_weight = trigular_weight
        self.loss = self.projection_loss if target == "projection" else self.mapping_loss
        self.criterion = nn.L1Loss() if loss_type == "l1" else nn.MSELoss()
        self.rec_weight = reconstruction_weight

    def projection_loss(self, fake_crossattn, real_crossattn, split="train", **kwargs):
        loss = self.criterion(real_crossattn, fake_crossattn)
        log = {
            "{}/total_loss".format(split): loss.clone().detach().mean(),
        }
        return loss, log

    def mapping_loss(self, x, sketch, fake_crossattn, real_crossattn, origin_crossattn, text_features, diffusion_model, split="train",
                     *args, **kwargs):
        # t = torch.randint(0, diffusion_model.num_timesteps, (x.shape[0],), device=x.device)
        t = torch.ones((x.shape[0],), device=x.device) * (diffusion_model.num_timesteps - 1)
        t = t.long()
        # x_noisy = diffusion_model.q_sample(x_start=x, t=t, noise=torch.randn_like(x))
        x = torch.randn_like(x)
        fake, fake_feats = diffusion_model.apply_model(x, t, {"c_concat": [sketch], "c_crossattn": [fake_crossattn[:, 1:]]}, return_feat=True)
        real, real_feats = diffusion_model.apply_model(x, t, {"c_concat": [sketch], "c_crossattn": [real_crossattn[:, 1:]]}, return_feat=True)
        real = diffusion_model.predict_start_from_noise(x, t, real)
        fake = diffusion_model.predict_start_from_noise(x, t, fake)

        feat_loss = 0.
        for real_feat, fake_feat in zip(real_feats, fake_feats):
            feat_loss += self.criterion(real_feat, fake_feat)
        rec_loss = self.criterion(real, fake)

        # inv_loss = self.criterion(gap(real_crossattn), gap(fake_crossattn))
        cos_loss = self.criterion(diffusion_model.cond_stage_model.calculate_scale(gap(real_crossattn[:, 1:]), text_features),
                                  diffusion_model.cond_stage_model.calculate_scale(gap(fake_crossattn[:, 1:]), text_features))

        # MSE-triangular loss
        # real_scale = gap(real_crossattn) @ text_features.mT
        # fake_scale = gap(fake_crossattn) @ text_features.mT
        #
        # cos_loss = torch.abs(real_scale ** 2. - fake_scale ** 2.)
        # sin_loss = sine_loss(gap(fake_crossattn), gap(origin_crossattn), text_features)
        # tri_loss = (cos_loss + sin_loss).sum() / x.shape[0]
        #
        loss = rec_loss + feat_loss + cos_loss
        log = {
            "{}/total_loss".format(split): loss.clone().detach().mean(),
            "{}/rec_loss".format(split): rec_loss.detach().mean(),
            "{}/feat_loss".format(split): feat_loss.detach().mean(),
            # "{}/inv_loss".format(split): inv_loss.detach().mean(),
            # "{}/tri_loss".format(split): tri_loss.detach().mean(),
            # "{}/sin_loss".format(split): sin_loss.detach().mean(),
            "{}/cos_loss".format(split): cos_loss.detach().mean(),
        }
        return loss, log

    def forward(self, *args, **kwargs):
        return self.loss(*args, **kwargs)